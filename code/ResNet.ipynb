{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic ResNet model\n",
    "\n",
    "def init_layer(L):\n",
    "    # Initialization using fan-in\n",
    "    if isinstance(L, nn.Conv2d):\n",
    "        n = L.kernel_size[0]*L.kernel_size[1]*L.out_channels\n",
    "        L.weight.data.normal_(0,math.sqrt(2.0/float(n)))\n",
    "    elif isinstance(L, nn.BatchNorm2d):\n",
    "        L.weight.data.fill_(1)\n",
    "        L.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Simple ResNet Block\n",
    "class SimpleBlock(nn.Module):\n",
    "    def __init__(self, indim, outdim, half_res):\n",
    "        super(SimpleBlock, self).__init__()\n",
    "        self.indim = indim\n",
    "        self.outdim = outdim\n",
    "        self.C1 = nn.Conv2d(indim, outdim, kernel_size=3, stride=2 if half_res else 1, padding=1, bias=False)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.BN1 = nn.BatchNorm2d(outdim)\n",
    "        self.C2 = nn.Conv2d(outdim, outdim,kernel_size=3, padding=1,bias=False)\n",
    "        self.BN2 = nn.BatchNorm2d(outdim)\n",
    "\n",
    "        self.parametrized_layers = [self.C1, self.C2, self.BN1, self.BN2]\n",
    "\n",
    "        self.half_res = half_res\n",
    "\n",
    "        # if the input number of channels is not equal to the output, then need a 1x1 convolution\n",
    "        if indim!=outdim:\n",
    "            self.shortcut = nn.Conv2d(indim, outdim, 1, 2 if half_res else 1, bias=False)\n",
    "            self.parametrized_layers.append(self.shortcut)\n",
    "            self.BNshortcut = nn.BatchNorm2d(outdim)\n",
    "            self.parametrized_layers.append(self.BNshortcut)\n",
    "            self.shortcut_type = '1x1'\n",
    "        else:\n",
    "            self.shortcut_type = 'identity'\n",
    "\n",
    "        for layer in self.parametrized_layers:\n",
    "            init_layer(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.C1(x)\n",
    "        out = self.BN1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.C2(out)\n",
    "        out = self.BN2(out)\n",
    "        short_out = x if self.shortcut_type == 'identity' else self.BNshortcut(self.shortcut(x))\n",
    "        out = out + short_out\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Bottleneck block\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, indim, outdim, half_res):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        bottleneckdim = int(outdim/4)\n",
    "        self.indim = indim\n",
    "        self.outdim = outdim\n",
    "        self.C1 = nn.Conv2d(indim, bottleneckdim, kernel_size=1,  bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.BN1 = nn.BatchNorm2d(bottleneckdim)\n",
    "        self.C2 = nn.Conv2d(bottleneckdim, bottleneckdim, kernel_size=3, stride=2 if half_res else 1,padding=1)\n",
    "        self.BN2 = nn.BatchNorm2d(bottleneckdim)\n",
    "        self.C3 = nn.Conv2d(bottleneckdim, outdim, kernel_size=1, bias=False)\n",
    "        self.BN3 = nn.BatchNorm2d(outdim)\n",
    "\n",
    "        self.parametrized_layers = [self.C1, self.BN1, self.C2, self.BN2, self.C3, self.BN3]\n",
    "        self.half_res = half_res\n",
    "\n",
    "\n",
    "        # if the input number of channels is not equal to the output, then need a 1x1 convolution\n",
    "        if indim!=outdim:\n",
    "            self.shortcut = nn.Conv2d(indim, outdim, 1, stride=2 if half_res else 1, bias=False)\n",
    "            self.parametrized_layers.append(self.shortcut)\n",
    "            self.shortcut_type = '1x1'\n",
    "        else:\n",
    "            self.shortcut_type = 'identity'\n",
    "\n",
    "        for layer in self.parametrized_layers:\n",
    "            init_layer(layer)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        short_out = x if self.shortcut_type == 'identity' else self.shortcut(x)\n",
    "        out = self.C1(x)\n",
    "        out = self.BN1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.C2(out)\n",
    "        out = self.BN2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.C3(out)\n",
    "        out = self.BN3(out)\n",
    "        out = out + short_out\n",
    "\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,block,list_of_num_layers, list_of_out_dims, num_classes=1000, only_trunk=False ):\n",
    "        # list_of_num_layers specifies number of layers in each stage\n",
    "        # list_of_out_dims specifies number of output channel for each stage\n",
    "        super(ResNet,self).__init__()\n",
    "        self.grads = []\n",
    "        self.fmaps = []\n",
    "        assert len(list_of_num_layers)==4, 'Can have only four stages'\n",
    "        conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                               bias=False)\n",
    "        bn1 = nn.BatchNorm2d(64)\n",
    "        relu = nn.ReLU()\n",
    "        pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        init_layer(conv1)\n",
    "        init_layer(bn1)\n",
    "\n",
    "\n",
    "        trunk = [conv1, bn1, relu, pool1]\n",
    "        indim = 64\n",
    "        for i in range(4):\n",
    "\n",
    "            for j in range(list_of_num_layers[i]):\n",
    "                half_res = (i>=1) and (j==0)\n",
    "                B = block(indim, list_of_out_dims[i], half_res)\n",
    "                trunk.append(B)\n",
    "                indim = list_of_out_dims[i]\n",
    "\n",
    "\n",
    "\n",
    "        self.only_trunk=only_trunk\n",
    "        if not only_trunk:\n",
    "            avgpool = nn.AvgPool2d(7)\n",
    "            trunk.append(avgpool)\n",
    "\n",
    "        self.trunk = nn.Sequential(*trunk)\n",
    "        self.final_feat_dim = indim\n",
    "        if not only_trunk:\n",
    "            self.classifier = nn.Linear(indim, num_classes)\n",
    "            self.classifier.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.trunk(x)\n",
    "        if self.only_trunk:\n",
    "            return out\n",
    "        out = out.view(out.size(0),-1)\n",
    "        scores = self.classifier(out)\n",
    "        return scores, out\n",
    "\n",
    "\n",
    "def ResNet10(num_classes=1000, only_trunk=False):\n",
    "    return ResNet(SimpleBlock, [1,1,1,1],[64,128,256,512], num_classes, only_trunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((300,300)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "train_dataset = datasets.ImageFolder(root='../aligned-data/train',\n",
    "                                           transform=data_transform)\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                             batch_size=8, shuffle=True, \n",
    "                                            num_workers=4)\n",
    "test_dataset = datasets.ImageFolder(root='../aligned-data/test',\n",
    "                                           transform=data_transform)\n",
    "test_dataset_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                             batch_size=90, shuffle=True,\n",
    "                                             num_workers=4)\n",
    "test_x, test_y = next(iter(test_dataset_loader))\n",
    "test_x, test_y = Variable(test_x), Variable(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet10(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(mdl, X, Y):\n",
    "    # TODO: why can't we call .data.numpy() for train_acc as a whole?\n",
    "    outputs, _ = mdl(X)\n",
    "    max_vals, max_indices = torch.max(outputs,1)\n",
    "    train_acc = (max_indices == Y).sum().data.numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  46.823426485061646\n",
      "Training accuracy:  [0.34347826]\n",
      "Test accuracy:  [0.35555556]\n",
      "Loss:  37.87575137615204\n",
      "Training accuracy:  [0.46304348]\n",
      "Test accuracy:  [0.44444444]\n",
      "Loss:  30.224955201148987\n",
      "Training accuracy:  [0.61304348]\n",
      "Test accuracy:  [0.52222222]\n",
      "Loss:  22.2606044113636\n",
      "Training accuracy:  [0.72826087]\n",
      "Test accuracy:  [0.61111111]\n",
      "Loss:  17.46561226248741\n",
      "Training accuracy:  [0.81521739]\n",
      "Test accuracy:  [0.68888889]\n",
      "Loss:  11.944070979952812\n",
      "Training accuracy:  [0.88043478]\n",
      "Test accuracy:  [0.72222222]\n",
      "Loss:  9.013656049966812\n",
      "Training accuracy:  [0.94130435]\n",
      "Test accuracy:  [0.78888889]\n",
      "Loss:  7.262864649295807\n",
      "Training accuracy:  [0.95869565]\n",
      "Test accuracy:  [0.75555556]\n",
      "Loss:  5.758986331522465\n",
      "Training accuracy:  [0.96956522]\n",
      "Test accuracy:  [0.8]\n",
      "Loss:  5.11051269993186\n",
      "Training accuracy:  [0.96956522]\n",
      "Test accuracy:  [0.76666667]\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    acc = 0\n",
    "    running_loss = 0.0\n",
    "    test_acc = 0\n",
    "    train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                             batch_size=20, shuffle=True,\n",
    "                                             num_workers=4)\n",
    "    \n",
    "    for i, data in enumerate(train_dataset_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, _ = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc += calc_accuracy(net, inputs, labels)\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        #if i % 5 == 4:    # print every 2000 mini-batches\n",
    "            #print('[%d, %5d] loss: %.3f' %\n",
    "            #      (epoch + 1, i + 1, running_loss / 5))\n",
    "        #    running_loss = 0.0\n",
    "        \n",
    "    # Compute test accuracy\n",
    "    test_acc += calc_accuracy(net, test_x, test_y)\n",
    "\n",
    "    print('Loss: ', running_loss)\n",
    "    print('Training accuracy: ', acc/(len(train_dataset_loader)))\n",
    "    print('Test accuracy: ', test_acc)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(model, data_loader, outfile ):\n",
    "\n",
    "    f = h5py.File(outfile, 'w')\n",
    "    max_count = len(data_loader)*data_loader.batch_size\n",
    "    all_labels = f.create_dataset('all_labels',(max_count,), dtype='i')\n",
    "    all_feats=None\n",
    "    count=0\n",
    "    for i, (x,y) in enumerate(data_loader):\n",
    "        if i%10 == 0:\n",
    "            print('{:d}/{:d}'.format(i, len(data_loader)))\n",
    "        x_var = Variable(x)\n",
    "        scores, feats = model(x_var)\n",
    "        if all_feats is None:\n",
    "            all_feats = f.create_dataset('all_feats', (max_count, feats.size(1)), dtype='f')\n",
    "        all_feats[count:count+feats.size(0),:] = feats.data.cpu().numpy()\n",
    "        all_labels[count:count+feats.size(0)] = y.cpu().numpy()\n",
    "        print(all_labels[count:count+feats.size(0)])\n",
    "        count = count + feats.size(0)\n",
    "\n",
    "    print(all_labels)\n",
    "    count_var = f.create_dataset('count', (1,), dtype='i')\n",
    "    count_var[0] = count\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/35\n",
      "[ 1  0  3  4  7  1 13  9 11  7  9  7  7 11  5  3  4  7  3  8]\n",
      "[11  1  1  8 11  2 10  0  9  9  8 11  7  5  9  9 11  3  3  3]\n",
      "[ 1 11 12 10  6 12  9  0  7  0  9 13  6 11  2  9  0  9 10  3]\n",
      "[ 2  6  5 12  3  7  4  2  0  9  6  8  0 13  5  9 13  2 13  8]\n",
      "[ 7 13 12  9 10  4 13  8  9  4  9  9  6 11  2  2  1  8  7 12]\n",
      "[ 2  1  6  2 10  1 10 13  2 12  3 10  3  9  8 13 13  5  1  7]\n",
      "[ 6  1  4  8  8 10  9  2  4  4  1  8 11  0 11 12  7  4  5 13]\n",
      "[10  1  3  9 10 11  1 11 10  5  4 11  4  5 11 13  0  6 13  3]\n",
      "[11 13 11 10  1  9  4 13  2 13 10  2 13  7  8 13  6 13  9  4]\n",
      "[10 11  2 11  9  7 10  1  7  6  5  0 12 12  0  7 10  6  7  8]\n",
      "10/35\n",
      "[ 5  0  5 10  3 12  0  6  4 13  8  5  4 13 11  2  7 10  5  9]\n",
      "[ 0  1  8 12 10  7 12  3  9  8  6  5 11 12 13  9  3  4 12  4]\n",
      "[13  9  8  8  8  6  6  7 13 10  3  0  1 11  6 12  6  0  6 12]\n",
      "[ 2 12  6  8  9  3  6 13  7  9 11 11  1  5  2 11 13  8  0  7]\n",
      "[10  2  7  1  0  4 10  8 11  6  5  8  1  8  3 10  1 10  4  8]\n",
      "[ 6  6  8  4  6  2  7 10  7 13 13  8  4  8  3  0  5  0 13 11]\n",
      "[ 3  3  5  3  7 11 12  0  7  2  9  0  1  5  8  3  2  6  3  0]\n",
      "[ 0  6  5  4  4  7  8  8  3 13  7  9 10  6  2 12 11 13  1  5]\n",
      "[12  1 12  1  0 13  7  9  1  6  9 13 12  6 10  7 11  3  3  4]\n",
      "[ 6 12  6 11  4 13  6  3  5 12  4  6  2  6  8  7  4 10 11 10]\n",
      "20/35\n",
      "[ 4  9 12  4 12 11 12 12  9  2  1  0  8  6  4 10 11  2 13  6]\n",
      "[13  4  8  8  1 10 12  9 12  9  1 12 10  6  8  3 10  0 10  8]\n",
      "[ 2  8  8  0  8  0  5 10 10  2  7  5  9  1  0 12 12  0 12 12]\n",
      "[10  0  3  7 11 13 10  6  8  2 10  2 12 11  7 11 13  0  5  5]\n",
      "[ 4 12 10  4  1  3  5 13 13  4  5 12  2  0 12  5 11  7  3 13]\n",
      "[11  5  4  1  5 12 11 12  2 13 13  8  7  2 11 10 10  9  1  0]\n",
      "[10  1  4  4  2  1  1  1  7  6 12  2  4 11  1  5  6  5  2  6]\n",
      "[11  7  3 12  1  2  6 11  3  9 13  3  4 13  8  8  5  2  3  3]\n",
      "[ 4  2  7  3 11  2  5  1  5  7  9 12  9  9 12  1  8 10  0  5]\n",
      "[ 9  3  2 10  4  4  6 12  5  4  5  2  5  5  0  0  6  9  3  7]\n",
      "30/35\n",
      "[11  1  2  0  5  5  0  9  8  3  6  1 11 10  0  4  1  4  6  3]\n",
      "[13 12  5 12  5  6  5  7  8  7 13  5  3  1  1 13  2  1  3 11]\n",
      "[ 6  1  1  7  3  0  3  8 10 13  3 13  8  7  9  3  3  8  4  5]\n",
      "[ 9  4  7 12  6  2  2  4 12 10  5  0  3  7  4  1  6 10 12  7]\n",
      "[ 5  9 11  9  0  2 11  9  2 10  7  4  2  2  2]\n",
      "<HDF5 dataset \"all_labels\": shape (700,), type \"<i4\">\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.ImageFolder(root='../novel-data/train',\n",
    "                                           transform=data_transform)\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                             batch_size=20, shuffle=True,\n",
    "                                             num_workers=4)\n",
    "save_features(net, train_dataset_loader, 'resnet_features.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(x, k, niter=1, batchsize=1000):\n",
    "    batchsize = min(batchsize, x.shape[0])\n",
    "\n",
    "    nsamples = x.shape[0]\n",
    "    ndims = x.shape[1]\n",
    "\n",
    "    x2 = np.sum(x**2, axis=1)\n",
    "    centroids = np.random.randn(k, ndims)\n",
    "    centroidnorm = np.sqrt(np.sum(centroids**2, axis=1, keepdims=True))\n",
    "    centroids = centroids / centroidnorm\n",
    "    totalcounts = np.zeros(k)\n",
    "\n",
    "    for i in range(niter):\n",
    "        c2 = np.sum(centroids**2, axis=1,keepdims=True)*0.5\n",
    "        summation = np.zeros((k, ndims))\n",
    "        counts = np.zeros(k)\n",
    "        loss = 0\n",
    "\n",
    "        for j in range(0, nsamples, batchsize):\n",
    "            lastj = min(j+batchsize, nsamples)\n",
    "            batch = x[j:lastj]\n",
    "            m = batch.shape[0]\n",
    "\n",
    "            tmp = np.dot(centroids, batch.T)\n",
    "            tmp = tmp - c2\n",
    "            val = np.max(tmp,0)\n",
    "            labels = np.argmax(tmp,0)\n",
    "            loss = loss + np.sum(np.sum(x2[j:lastj])*0.5 - val)\n",
    "\n",
    "            S = np.zeros((k, m))\n",
    "            S[labels, np.arange(m)] = 1\n",
    "            summation = summation + np.dot(S, batch)\n",
    "            counts = counts + np.sum(S, axis=1)\n",
    "\n",
    "        for j in range(k):\n",
    "            if counts[j]>0:\n",
    "                centroids[j] = summation[j] / counts[j]\n",
    "\n",
    "        totalcounts = totalcounts + counts\n",
    "        for j in range(k):\n",
    "            if totalcounts[j] == 0:\n",
    "                idx = np.random.choice(nsamples)\n",
    "                centroids[j] = x[idx]\n",
    "\n",
    "\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_feats(filehandle, base_classes, cachefile, n_clusters=5):\n",
    "    if os.path.isfile(cachefile):\n",
    "        with open(cachefile, 'rb') as f:\n",
    "            centroids = pickle.load(f)\n",
    "    else:\n",
    "        centroids = []\n",
    "        all_labels = filehandle['all_labels'][...]\n",
    "        all_feats = filehandle['all_feats']\n",
    "\n",
    "        count = filehandle['count'][0]\n",
    "        for j, i in enumerate(base_classes):\n",
    "            print('Clustering class {:d}:{:d}'.format(j,i))\n",
    "            idx = np.where(all_labels==i)[0]\n",
    "            idx = idx[idx<count]\n",
    "            X = all_feats[idx,:]\n",
    "            # use a reimplementation of torch kmeans for reproducible results\n",
    "            # TODO: Figure out why this is important\n",
    "            centroids_this = kmeans(X, n_clusters, 20)\n",
    "            centroids.append(centroids_this)\n",
    "        with open(cachefile, 'wb') as f:\n",
    "            pickle.dump(centroids, f)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering class 0:0\n",
      "Clustering class 1:1\n",
      "Clustering class 2:2\n",
      "Clustering class 3:3\n",
      "Clustering class 4:4\n",
      "Clustering class 5:5\n",
      "Clustering class 6:6\n",
      "Clustering class 7:7\n",
      "Clustering class 8:8\n"
     ]
    }
   ],
   "source": [
    "base_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "novel_classes = [9, 10, 11, 12, 13]\n",
    "with h5py.File('resnet_features.hdf5', 'r') as features_file:\n",
    "    centroids = cluster_feats(features_file, base_classes, 'centroids.pkl', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mine Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_difference_vectors(c_i):\n",
    "    diff_i = c_i[:,np.newaxis,:] - c_i[np.newaxis,:,:]\n",
    "    diff_i = diff_i.reshape((-1, diff_i.shape[2]))\n",
    "    diff_i_norm = np.sqrt(np.sum(diff_i**2,axis=1, keepdims=True))\n",
    "    diff_i = diff_i / (diff_i_norm + 0.00001)\n",
    "    return diff_i\n",
    "\n",
    "def mine_analogies(centroids):\n",
    "    n_clusters = centroids[0].shape[0]\n",
    "\n",
    "    analogies = np.zeros((n_clusters*n_clusters*len(centroids),4), dtype=int)\n",
    "    analogy_scores = np.zeros(analogies.shape[0])\n",
    "    start=0\n",
    "\n",
    "    I, J = np.unravel_index(np.arange(n_clusters**2), (n_clusters, n_clusters))\n",
    "    # for every class\n",
    "    for i, c_i in enumerate(centroids):\n",
    "\n",
    "        # get normalized difference vectors between cluster centers\n",
    "        diff_i = get_difference_vectors(c_i)\n",
    "        diff_i_t = torch.Tensor(diff_i)\n",
    "\n",
    "\n",
    "        bestdots = np.zeros(diff_i.shape[0])\n",
    "        bestdotidx = np.zeros((diff_i.shape[0],2),dtype=int)\n",
    "\n",
    "        # for every other class\n",
    "        for j, c_j in enumerate(centroids):\n",
    "            if i==j:\n",
    "                continue\n",
    "            print(i,j)\n",
    "\n",
    "            # get normalized difference vectors\n",
    "            diff_j = get_difference_vectors(c_j)\n",
    "            diff_j = torch.Tensor(diff_j)\n",
    "\n",
    "            #compute cosine distance and take the maximum\n",
    "            dots = diff_i_t.mm(diff_j.transpose(0,1))\n",
    "            maxdots, argmaxdots = dots.max(1)\n",
    "            maxdots = maxdots.cpu().numpy().reshape(-1)\n",
    "            argmaxdots = argmaxdots.cpu().numpy().reshape(-1)\n",
    "\n",
    "            # if maximum is better than best seen so far, update\n",
    "            betteridx = maxdots>bestdots\n",
    "            bestdots[betteridx] = maxdots[betteridx]\n",
    "            bestdotidx[betteridx,0] = j*n_clusters + I[argmaxdots[betteridx]]\n",
    "            bestdotidx[betteridx,1] = j*n_clusters + J[argmaxdots[betteridx]]\n",
    "\n",
    "\n",
    "        # store discovered analogies\n",
    "        stop = start+diff_i.shape[0]\n",
    "        analogies[start : stop,0]=i*n_clusters + I\n",
    "        analogies[start : stop,1]=i*n_clusters + J\n",
    "        analogies[start : stop,2:] = bestdotidx\n",
    "        analogy_scores[start : stop] = bestdots\n",
    "        start = stop\n",
    "\n",
    "    #prune away trivial analogies\n",
    "    good_analogies = (analogy_scores>0) & (analogies[:,0]!=analogies[:,1]) & (analogies[:,2]!=analogies[:,3])\n",
    "    return analogies[good_analogies,:], analogy_scores[good_analogies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "0 5\n",
      "0 6\n",
      "0 7\n",
      "0 8\n",
      "1 0\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "1 5\n",
      "1 6\n",
      "1 7\n",
      "1 8\n",
      "2 0\n",
      "2 1\n",
      "2 3\n",
      "2 4\n",
      "2 5\n",
      "2 6\n",
      "2 7\n",
      "2 8\n",
      "3 0\n",
      "3 1\n",
      "3 2\n",
      "3 4\n",
      "3 5\n",
      "3 6\n",
      "3 7\n",
      "3 8\n",
      "4 0\n",
      "4 1\n",
      "4 2\n",
      "4 3\n",
      "4 5\n",
      "4 6\n",
      "4 7\n",
      "4 8\n",
      "5 0\n",
      "5 1\n",
      "5 2\n",
      "5 3\n",
      "5 4\n",
      "5 6\n",
      "5 7\n",
      "5 8\n",
      "6 0\n",
      "6 1\n",
      "6 2\n",
      "6 3\n",
      "6 4\n",
      "6 5\n",
      "6 7\n",
      "6 8\n",
      "7 0\n",
      "7 1\n",
      "7 2\n",
      "7 3\n",
      "7 4\n",
      "7 5\n",
      "7 6\n",
      "7 8\n",
      "8 0\n",
      "8 1\n",
      "8 2\n",
      "8 3\n",
      "8 4\n",
      "8 5\n",
      "8 6\n",
      "8 7\n"
     ]
    }
   ],
   "source": [
    "analogies, analogy_scores = mine_analogies(centroids)\n",
    "np.save('analogies.npy', analogies.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(filehandle, base_classes, cachefile, networkfile, total_num_classes=14, lr=0.1, wd=0.0001, momentum=0.9, batchsize=20, niter=500):\n",
    "    # either use pre-existing classifier or train one\n",
    "    all_labels = filehandle['all_labels'][...]\n",
    "    all_labels = all_labels.astype(int)\n",
    "    all_feats = filehandle['all_feats']\n",
    "    base_class_ids = np.where(np.in1d(all_labels, base_classes))[0]\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    model = nn.Linear(all_feats[0].size, total_num_classes)\n",
    "    if os.path.isfile(cachefile):\n",
    "        tmp = torch.load(cachefile)\n",
    "        model.load_state_dict(tmp)\n",
    "    elif os.path.isfile(networkfile):\n",
    "        tmp = torch.load(networkfile)\n",
    "        if 'module.classifier.bias' in tmp['state']:\n",
    "            state_dict = {'weight':tmp['state']['module.classifier.weight'], 'bias':tmp['state']['module.classifier.bias']}\n",
    "        else:\n",
    "            model = nn.Linear(all_feats[0].size, total_num_classes, bias=False).cuda()\n",
    "            state_dict = {'weight':tmp['state']['module.classifier.weight']}\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr, momentum=momentum, weight_decay=wd, dampening=0)\n",
    "        for i in range(niter):\n",
    "            optimizer.zero_grad()\n",
    "            idx = np.sort(np.random.choice(base_class_ids, batchsize, replace=False))\n",
    "            F = all_feats[idx,:]\n",
    "            F = Variable(torch.Tensor(F))\n",
    "            L = Variable(torch.LongTensor(all_labels[idx]))\n",
    "            S = model(F)\n",
    "            loss_val = loss(S, L)\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                print('Classifier training {:d}: {:f}'.format(i, loss_val.data[0]))\n",
    "        torch.save(model.state_dict(), cachefile)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier training 0: 2.664518\n",
      "Classifier training 100: 0.767853\n",
      "Classifier training 200: 0.000521\n",
      "Classifier training 300: 0.000106\n",
      "Classifier training 400: 0.000510\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('resnet_features.hdf5', 'r') as features_file:\n",
    "    classification_model = train_classifier(features_file, base_classes, 'classifier.pkl', 'random.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy regressor train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalogyRegressor(nn.Module):\n",
    "    def __init__(self, featdim, innerdim=512):\n",
    "        super(AnalogyRegressor,self).__init__()\n",
    "        self.featdim = featdim\n",
    "        self.innerdim = innerdim\n",
    "        self.fc1 = nn.Linear(featdim*3, innerdim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(innerdim, innerdim)\n",
    "        self.fc3 = nn.Linear(innerdim, featdim)\n",
    "\n",
    "    def forward(self, a,c,d):\n",
    "        x = torch.cat((a,c,d), dim=1)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "def train_analogy_regressor(analogies, centroids, base_classes, trained_classifier, lr=0.1, wt=10, niter=5000, step_after=5000, batchsize=40, momentum=0.9, wd=0.0001):\n",
    "    # pre-permute analogies\n",
    "    permuted_analogies = analogies[np.random.permutation(analogies.shape[0])]\n",
    "\n",
    "    # create model and init\n",
    "    featdim = centroids[0].shape[1]\n",
    "    model = AnalogyRegressor(featdim)\n",
    "    model = model\n",
    "    trained_classifier = trained_classifier\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr, momentum=momentum, weight_decay=wd, dampening=momentum)\n",
    "    loss_1 = nn.CrossEntropyLoss()\n",
    "    loss_2 = nn.MSELoss()\n",
    "\n",
    "\n",
    "    num_clusters_per_class = centroids[0].shape[0]\n",
    "    centroid_labels = (np.array(base_classes).reshape((-1,1)) * np.ones((1, num_clusters_per_class))).reshape(-1)\n",
    "    concatenated_centroids = np.concatenate(centroids, axis=0)\n",
    "\n",
    "\n",
    "    start=0\n",
    "    avg_loss_1 = avg_loss_2 = count = 0.0\n",
    "    for i in range(niter):\n",
    "        # get current batch of analogies\n",
    "        stop = min(start+batchsize, permuted_analogies.shape[0])\n",
    "        #print(start+batchsize, permuted_analogies.shape[0])\n",
    "        to_train = permuted_analogies[start:stop,:]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # analogy is A:B :: C:D, goal is to predict B from A, C, D\n",
    "        # Y is the class label of B (and A)\n",
    "        A = concatenated_centroids[to_train[:,0]]\n",
    "        B = concatenated_centroids[to_train[:,1]]\n",
    "        C = concatenated_centroids[to_train[:,2]]\n",
    "        D = concatenated_centroids[to_train[:,3]]\n",
    "        Y = centroid_labels[to_train[:,1]]\n",
    "\n",
    "        A = Variable(torch.Tensor(A))\n",
    "        B = Variable(torch.Tensor(B))\n",
    "        C = Variable(torch.Tensor(C))\n",
    "        D = Variable(torch.Tensor(D))\n",
    "        Y = Variable(torch.LongTensor(Y.astype(int)))\n",
    "\n",
    "        Bhat = model(A,C,D)\n",
    "\n",
    "        lossval_2 = loss_2(Bhat, B) # simple mean squared error loss\n",
    "\n",
    "        # classification loss\n",
    "        predicted_classprobs = trained_classifier(Bhat)\n",
    "        lossval_1 = loss_1(predicted_classprobs, Y)\n",
    "        loss = lossval_1 + wt * lossval_2\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss_1 = avg_loss_1 + lossval_1.data[0]\n",
    "        avg_loss_2 = avg_loss_2 + lossval_2.data[0]\n",
    "        count = count+1.0\n",
    "\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('{:d} : {:f}, {:f}, {:f}'.format(i, avg_loss_1/count, avg_loss_2/count, count))\n",
    "            avg_loss_1 = avg_loss_2 = count = 0.0\n",
    "\n",
    "        if (i+1) % step_after == 0:\n",
    "            lr = lr / 10.0\n",
    "            print(lr)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "        start = stop\n",
    "        if start==permuted_analogies.shape[0]:\n",
    "            start=0\n",
    "\n",
    "    return dict(model_state=model.state_dict(), concatenated_centroids=torch.Tensor(concatenated_centroids),\n",
    "            num_base_classes=len(centroids), num_clusters_per_class=num_clusters_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "print(analogies.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 3.284535, 0.413273, 1.000000\n",
      "50 : 4.267699, 0.371373, 50.000000\n",
      "100 : 1.325531, 0.257671, 50.000000\n",
      "150 : 0.574396, 0.189520, 50.000000\n",
      "200 : 0.082800, 0.148434, 50.000000\n",
      "250 : 0.019898, 0.125422, 50.000000\n",
      "300 : 0.019362, 0.115872, 50.000000\n",
      "350 : 0.002575, 0.109401, 50.000000\n",
      "400 : 0.002543, 0.107121, 50.000000\n",
      "450 : 0.002537, 0.105595, 50.000000\n",
      "500 : 0.002618, 0.103619, 50.000000\n",
      "550 : 0.002618, 0.101775, 50.000000\n",
      "600 : 0.002523, 0.100765, 50.000000\n",
      "650 : 0.002433, 0.099796, 50.000000\n",
      "700 : 0.002405, 0.098065, 50.000000\n",
      "750 : 0.002250, 0.096721, 50.000000\n",
      "800 : 0.002110, 0.095772, 50.000000\n",
      "850 : 0.001966, 0.093493, 50.000000\n",
      "900 : 0.001882, 0.092033, 50.000000\n",
      "950 : 0.001823, 0.090690, 50.000000\n",
      "1000 : 0.001753, 0.090101, 50.000000\n",
      "1050 : 0.001709, 0.089590, 50.000000\n",
      "1100 : 0.001679, 0.089119, 50.000000\n",
      "1150 : 0.001665, 0.088665, 50.000000\n",
      "1200 : 0.001648, 0.088224, 50.000000\n",
      "1250 : 0.001623, 0.087804, 50.000000\n",
      "1300 : 0.001590, 0.087399, 50.000000\n",
      "1350 : 0.001577, 0.087006, 50.000000\n",
      "1400 : 0.001555, 0.086617, 50.000000\n",
      "1450 : 0.001538, 0.086134, 50.000000\n",
      "1500 : 0.001471, 0.085485, 50.000000\n",
      "1550 : 0.001455, 0.085092, 50.000000\n",
      "1600 : 0.001429, 0.084734, 50.000000\n",
      "1650 : 0.001406, 0.084390, 50.000000\n",
      "1700 : 0.001380, 0.084062, 50.000000\n",
      "1750 : 0.001355, 0.083747, 50.000000\n",
      "1800 : 0.001328, 0.083443, 50.000000\n",
      "1850 : 0.001302, 0.083149, 50.000000\n",
      "1900 : 0.001275, 0.082860, 50.000000\n",
      "1950 : 0.001255, 0.082312, 50.000000\n",
      "2000 : 0.001205, 0.081557, 50.000000\n",
      "2050 : 0.001187, 0.081046, 50.000000\n",
      "2100 : 0.001164, 0.080779, 50.000000\n",
      "2150 : 0.001146, 0.080534, 50.000000\n",
      "2200 : 0.001129, 0.080304, 50.000000\n",
      "2250 : 0.001111, 0.080081, 50.000000\n",
      "2300 : 0.001105, 0.079865, 50.000000\n",
      "2350 : 0.001081, 0.079659, 50.000000\n",
      "2400 : 0.001070, 0.079459, 50.000000\n",
      "2450 : 0.001067, 0.078896, 50.000000\n",
      "2500 : 0.001034, 0.078086, 50.000000\n",
      "2550 : 0.001017, 0.077855, 50.000000\n",
      "2600 : 0.001001, 0.077501, 50.000000\n",
      "2650 : 0.001041, 0.076272, 50.000000\n",
      "2700 : 0.001006, 0.076008, 50.000000\n",
      "2750 : 0.000989, 0.075828, 50.000000\n",
      "2800 : 0.000967, 0.075661, 50.000000\n",
      "2850 : 0.000955, 0.075501, 50.000000\n",
      "2900 : 0.000940, 0.075349, 50.000000\n",
      "2950 : 0.000941, 0.075203, 50.000000\n",
      "3000 : 0.000925, 0.075009, 50.000000\n",
      "3050 : 0.000912, 0.074509, 50.000000\n",
      "3100 : 0.000940, 0.074173, 50.000000\n",
      "3150 : 0.000930, 0.073811, 50.000000\n",
      "3200 : 0.000908, 0.073317, 50.000000\n",
      "3250 : 0.000899, 0.072901, 50.000000\n",
      "3300 : 0.000885, 0.072732, 50.000000\n",
      "3350 : 0.000876, 0.072593, 50.000000\n",
      "3400 : 0.000866, 0.072469, 50.000000\n",
      "3450 : 0.000848, 0.072350, 50.000000\n",
      "3500 : 0.000836, 0.072240, 50.000000\n",
      "3550 : 0.000840, 0.072132, 50.000000\n",
      "3600 : 0.000839, 0.071844, 50.000000\n",
      "3650 : 0.000853, 0.071167, 50.000000\n",
      "3700 : 0.000837, 0.070977, 50.000000\n",
      "3750 : 0.000834, 0.070855, 50.000000\n",
      "3800 : 0.000829, 0.070750, 50.000000\n",
      "3850 : 0.000806, 0.070655, 50.000000\n",
      "3900 : 0.000815, 0.070559, 50.000000\n",
      "3950 : 0.000809, 0.070476, 50.000000\n",
      "4000 : 0.000789, 0.070387, 50.000000\n",
      "4050 : 0.000789, 0.070306, 50.000000\n",
      "4100 : 0.000791, 0.070215, 50.000000\n",
      "4150 : 0.000778, 0.070134, 50.000000\n",
      "4200 : 0.000770, 0.070057, 50.000000\n",
      "4250 : 0.000776, 0.069970, 50.000000\n",
      "4300 : 0.000757, 0.069904, 50.000000\n",
      "4350 : 0.000760, 0.069825, 50.000000\n",
      "4400 : 0.000759, 0.069753, 50.000000\n",
      "4450 : 0.000740, 0.069680, 50.000000\n",
      "4500 : 0.000746, 0.069605, 50.000000\n",
      "4550 : 0.000735, 0.069546, 50.000000\n",
      "4600 : 0.000733, 0.069479, 50.000000\n",
      "4650 : 0.000732, 0.069408, 50.000000\n",
      "4700 : 0.000715, 0.069350, 50.000000\n",
      "4750 : 0.000728, 0.069281, 50.000000\n",
      "4800 : 0.000707, 0.069224, 50.000000\n",
      "4850 : 0.000718, 0.069159, 50.000000\n",
      "4900 : 0.000698, 0.069105, 50.000000\n",
      "4950 : 0.000711, 0.069044, 50.000000\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "generator = train_analogy_regressor(analogies, centroids, base_classes, classification_model, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(generator['num_base_classes'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
