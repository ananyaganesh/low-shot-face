{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic ResNet model\n",
    "\n",
    "def init_layer(L):\n",
    "    # Initialization using fan-in\n",
    "    if isinstance(L, nn.Conv2d):\n",
    "        n = L.kernel_size[0]*L.kernel_size[1]*L.out_channels\n",
    "        L.weight.data.normal_(0,math.sqrt(2.0/float(n)))\n",
    "    elif isinstance(L, nn.BatchNorm2d):\n",
    "        L.weight.data.fill_(1)\n",
    "        L.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Simple ResNet Block\n",
    "class SimpleBlock(nn.Module):\n",
    "    def __init__(self, indim, outdim, half_res):\n",
    "        super(SimpleBlock, self).__init__()\n",
    "        self.indim = indim\n",
    "        self.outdim = outdim\n",
    "        self.C1 = nn.Conv2d(indim, outdim, kernel_size=3, stride=2 if half_res else 1, padding=1, bias=False)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.BN1 = nn.BatchNorm2d(outdim)\n",
    "        self.C2 = nn.Conv2d(outdim, outdim,kernel_size=3, padding=1,bias=False)\n",
    "        self.BN2 = nn.BatchNorm2d(outdim)\n",
    "\n",
    "        self.parametrized_layers = [self.C1, self.C2, self.BN1, self.BN2]\n",
    "\n",
    "        self.half_res = half_res\n",
    "\n",
    "        # if the input number of channels is not equal to the output, then need a 1x1 convolution\n",
    "        if indim!=outdim:\n",
    "            self.shortcut = nn.Conv2d(indim, outdim, 1, 2 if half_res else 1, bias=False)\n",
    "            self.parametrized_layers.append(self.shortcut)\n",
    "            self.BNshortcut = nn.BatchNorm2d(outdim)\n",
    "            self.parametrized_layers.append(self.BNshortcut)\n",
    "            self.shortcut_type = '1x1'\n",
    "        else:\n",
    "            self.shortcut_type = 'identity'\n",
    "\n",
    "        for layer in self.parametrized_layers:\n",
    "            init_layer(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.C1(x)\n",
    "        out = self.BN1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.C2(out)\n",
    "        out = self.BN2(out)\n",
    "        short_out = x if self.shortcut_type == 'identity' else self.BNshortcut(self.shortcut(x))\n",
    "        out = out + short_out\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Bottleneck block\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, indim, outdim, half_res):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        bottleneckdim = int(outdim/4)\n",
    "        self.indim = indim\n",
    "        self.outdim = outdim\n",
    "        self.C1 = nn.Conv2d(indim, bottleneckdim, kernel_size=1,  bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.BN1 = nn.BatchNorm2d(bottleneckdim)\n",
    "        self.C2 = nn.Conv2d(bottleneckdim, bottleneckdim, kernel_size=3, stride=2 if half_res else 1,padding=1)\n",
    "        self.BN2 = nn.BatchNorm2d(bottleneckdim)\n",
    "        self.C3 = nn.Conv2d(bottleneckdim, outdim, kernel_size=1, bias=False)\n",
    "        self.BN3 = nn.BatchNorm2d(outdim)\n",
    "\n",
    "        self.parametrized_layers = [self.C1, self.BN1, self.C2, self.BN2, self.C3, self.BN3]\n",
    "        self.half_res = half_res\n",
    "\n",
    "\n",
    "        # if the input number of channels is not equal to the output, then need a 1x1 convolution\n",
    "        if indim!=outdim:\n",
    "            self.shortcut = nn.Conv2d(indim, outdim, 1, stride=2 if half_res else 1, bias=False)\n",
    "            self.parametrized_layers.append(self.shortcut)\n",
    "            self.shortcut_type = '1x1'\n",
    "        else:\n",
    "            self.shortcut_type = 'identity'\n",
    "\n",
    "        for layer in self.parametrized_layers:\n",
    "            init_layer(layer)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        short_out = x if self.shortcut_type == 'identity' else self.shortcut(x)\n",
    "        out = self.C1(x)\n",
    "        out = self.BN1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.C2(out)\n",
    "        out = self.BN2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.C3(out)\n",
    "        out = self.BN3(out)\n",
    "        out = out + short_out\n",
    "\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,block,list_of_num_layers, list_of_out_dims, num_classes=1000, only_trunk=False ):\n",
    "        # list_of_num_layers specifies number of layers in each stage\n",
    "        # list_of_out_dims specifies number of output channel for each stage\n",
    "        super(ResNet,self).__init__()\n",
    "        self.grads = []\n",
    "        self.fmaps = []\n",
    "        assert len(list_of_num_layers)==4, 'Can have only four stages'\n",
    "        conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                               bias=False)\n",
    "        bn1 = nn.BatchNorm2d(64)\n",
    "        relu = nn.ReLU()\n",
    "        pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        init_layer(conv1)\n",
    "        init_layer(bn1)\n",
    "\n",
    "\n",
    "        trunk = [conv1, bn1, relu, pool1]\n",
    "        indim = 64\n",
    "        for i in range(4):\n",
    "\n",
    "            for j in range(list_of_num_layers[i]):\n",
    "                half_res = (i>=1) and (j==0)\n",
    "                B = block(indim, list_of_out_dims[i], half_res)\n",
    "                trunk.append(B)\n",
    "                indim = list_of_out_dims[i]\n",
    "\n",
    "        self.only_trunk=only_trunk\n",
    "        if not only_trunk:\n",
    "            avgpool = nn.AvgPool2d(7)\n",
    "            trunk.append(avgpool)\n",
    "\n",
    "        self.trunk = nn.Sequential(*trunk)\n",
    "        self.final_feat_dim = indim\n",
    "        if not only_trunk:\n",
    "            self.classifier = nn.Linear(indim, num_classes)\n",
    "            self.classifier.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.trunk(x)\n",
    "        if self.only_trunk:\n",
    "            return out\n",
    "        out = out.view(out.size(0),-1)\n",
    "        scores = self.classifier(out)\n",
    "        return scores, out\n",
    "\n",
    "\n",
    "def ResNet10(num_classes=1000, only_trunk=False):\n",
    "    return ResNet(SimpleBlock, [1,1,1,1],[64,128,256,512], num_classes, only_trunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((300,300)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "train_dataset = datasets.ImageFolder(root='../novel-data/train',\n",
    "                                           transform=data_transform)\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                             batch_size=20, shuffle=True, \n",
    "                                            num_workers=4)\n",
    "test_dataset = datasets.ImageFolder(root='../novel-data/test',\n",
    "                                           transform=data_transform)\n",
    "test_dataset_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                             batch_size=140, shuffle=True,\n",
    "                                             num_workers=4)\n",
    "test_x, test_y = next(iter(test_dataset_loader))\n",
    "test_x, test_y = Variable(test_x), Variable(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet10(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(mdl, X, Y):\n",
    "    # TODO: why can't we call .data.numpy() for train_acc as a whole?\n",
    "    outputs, _ = mdl(X)\n",
    "    max_vals, max_indices = torch.max(outputs,1)\n",
    "    train_acc = (max_indices == Y).sum().data.numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  34.1836621761322\n",
      "Training accuracy:  [0.62037037]\n",
      "Loss:  26.584738075733185\n",
      "Training accuracy:  [0.69398148]\n",
      "Loss:  22.328427612781525\n",
      "Training accuracy:  [0.75532407]\n",
      "Loss:  14.412934720516205\n",
      "Training accuracy:  [0.87083333]\n",
      "Loss:  10.425279274582863\n",
      "Training accuracy:  [0.92662037]\n",
      "Loss:  8.8801826313138\n",
      "Training accuracy:  [0.94375]\n",
      "Loss:  6.465670563280582\n",
      "Training accuracy:  [0.9625]\n",
      "Loss:  4.982062846422195\n",
      "Training accuracy:  [0.98333333]\n",
      "Loss:  3.6035365387797356\n",
      "Training accuracy:  [0.99166667]\n",
      "Loss:  3.101884040981531\n",
      "Training accuracy:  [0.98958333]\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    acc = 0\n",
    "    running_loss = 0.0\n",
    "    test_acc = 0\n",
    "    train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                             batch_size=20, shuffle=True,\n",
    "                                             num_workers=4)\n",
    "    \n",
    "    for i, data in enumerate(train_dataset_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, _ = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc += calc_accuracy(net, inputs, labels)\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        #if i % 5 == 4:    # print every 2000 mini-batches\n",
    "            #print('[%d, %5d] loss: %.3f' %\n",
    "            #      (epoch + 1, i + 1, running_loss / 5))\n",
    "        #    running_loss = 0.0\n",
    "        \n",
    "    # Compute test accuracy\n",
    "    #test_acc += calc_accuracy(net, test_x, test_y)\n",
    "\n",
    "    print('Loss: ', running_loss)\n",
    "    print('Training accuracy: ', acc/(len(train_dataset_loader)))\n",
    "    #print('Test accuracy: ', test_acc)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58992806]\n"
     ]
    }
   ],
   "source": [
    "print(calc_accuracy(net, test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(model, data_loader, outfile ):\n",
    "\n",
    "    f = h5py.File(outfile, 'w')\n",
    "    max_count = len(data_loader)*data_loader.batch_size\n",
    "    all_labels = f.create_dataset('all_labels',(max_count,), dtype='i')\n",
    "    all_feats=None\n",
    "    count=0\n",
    "    for i, (x,y) in enumerate(data_loader):\n",
    "        #print(y)\n",
    "        if i%10 == 0:\n",
    "            print('{:d}/{:d}'.format(i, len(data_loader)))\n",
    "        x_var = Variable(x)\n",
    "        scores, feats = model(x_var)\n",
    "        if all_feats is None:\n",
    "            all_feats = f.create_dataset('all_feats', (max_count, feats.size(1)), dtype='f')\n",
    "        all_feats[count:count+feats.size(0),:] = feats.data.cpu().numpy()\n",
    "        all_labels[count:count+feats.size(0)] = y.cpu().numpy()\n",
    "        #print(all_labels[count:count+feats.size(0)])\n",
    "        count = count + feats.size(0)\n",
    "\n",
    "    #print(all_labels)\n",
    "    count_var = f.create_dataset('count', (1,), dtype='i')\n",
    "    count_var[0] = count\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root='../novel-data/train',\n",
    "                                           transform=data_transform)\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                             batch_size=20, shuffle=True,\n",
    "                                             num_workers=4)\n",
    "#pics, cls = next(iter(train_dataset_loader))\n",
    "class_names = train_dataset.classes\n",
    "#for x in cls:\n",
    "#    print(x, class_names[x])\n",
    "save_features(net, train_dataset_loader, 'resnet_features.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classes = [0,6,7,8,9,10,11,12,13]\n",
    "novel_classes = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(x, k, niter=1, batchsize=1000):\n",
    "    batchsize = min(batchsize, x.shape[0])\n",
    "\n",
    "    nsamples = x.shape[0]\n",
    "    ndims = x.shape[1]\n",
    "\n",
    "    x2 = np.sum(x**2, axis=1)\n",
    "    centroids = np.random.randn(k, ndims)\n",
    "    centroidnorm = np.sqrt(np.sum(centroids**2, axis=1, keepdims=True))\n",
    "    centroids = centroids / centroidnorm\n",
    "    totalcounts = np.zeros(k)\n",
    "\n",
    "    for i in range(niter):\n",
    "        c2 = np.sum(centroids**2, axis=1,keepdims=True)*0.5\n",
    "        summation = np.zeros((k, ndims))\n",
    "        counts = np.zeros(k)\n",
    "        loss = 0\n",
    "\n",
    "        for j in range(0, nsamples, batchsize):\n",
    "            lastj = min(j+batchsize, nsamples)\n",
    "            batch = x[j:lastj]\n",
    "            m = batch.shape[0]\n",
    "\n",
    "            tmp = np.dot(centroids, batch.T)\n",
    "            tmp = tmp - c2\n",
    "            val = np.max(tmp,0)\n",
    "            labels = np.argmax(tmp,0)\n",
    "            loss = loss + np.sum(np.sum(x2[j:lastj])*0.5 - val)\n",
    "\n",
    "            S = np.zeros((k, m))\n",
    "            S[labels, np.arange(m)] = 1\n",
    "            summation = summation + np.dot(S, batch)\n",
    "            counts = counts + np.sum(S, axis=1)\n",
    "\n",
    "        for j in range(k):\n",
    "            if counts[j]>0:\n",
    "                centroids[j] = summation[j] / counts[j]\n",
    "\n",
    "        totalcounts = totalcounts + counts\n",
    "        for j in range(k):\n",
    "            if totalcounts[j] == 0:\n",
    "                idx = np.random.choice(nsamples)\n",
    "                centroids[j] = x[idx]\n",
    "\n",
    "\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_feats(filehandle, base_classes, cachefile, n_clusters=5):\n",
    "    if os.path.isfile(cachefile):\n",
    "        with open(cachefile, 'rb') as f:\n",
    "            centroids = pickle.load(f)\n",
    "    else:\n",
    "        centroids = []\n",
    "        all_labels = filehandle['all_labels'][...]\n",
    "        all_feats = filehandle['all_feats']\n",
    "\n",
    "        count = filehandle['count'][0]\n",
    "        for j, i in enumerate(base_classes):\n",
    "            print('Clustering class {:d}:{:d}'.format(j,i))\n",
    "            idx = np.where(all_labels==i)[0]\n",
    "            idx = idx[idx<count]\n",
    "            X = all_feats[idx,:]\n",
    "            # use a reimplementation of torch kmeans for reproducible results\n",
    "            # TODO: Figure out why this is important\n",
    "            centroids_this = kmeans(X, n_clusters, 20)\n",
    "            centroids.append(centroids_this)\n",
    "        with open(cachefile, 'wb') as f:\n",
    "            pickle.dump(centroids, f)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering class 0:0\n",
      "Clustering class 1:6\n",
      "Clustering class 2:7\n",
      "Clustering class 3:8\n",
      "Clustering class 4:9\n",
      "Clustering class 5:10\n",
      "Clustering class 6:11\n",
      "Clustering class 7:12\n",
      "Clustering class 8:13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with h5py.File('resnet_features.hdf5', 'r') as features_file:\n",
    "    centroids = cluster_feats(features_file, base_classes, 'centroids.pkl', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mine Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_difference_vectors(c_i):\n",
    "    diff_i = c_i[:,np.newaxis,:] - c_i[np.newaxis,:,:]\n",
    "    diff_i = diff_i.reshape((-1, diff_i.shape[2]))\n",
    "    diff_i_norm = np.sqrt(np.sum(diff_i**2,axis=1, keepdims=True))\n",
    "    diff_i = diff_i / (diff_i_norm + 0.00001)\n",
    "    return diff_i\n",
    "\n",
    "def mine_analogies(centroids):\n",
    "    n_clusters = centroids[0].shape[0]\n",
    "\n",
    "    analogies = np.zeros((n_clusters*n_clusters*len(centroids),4), dtype=int)\n",
    "    analogy_scores = np.zeros(analogies.shape[0])\n",
    "    start=0\n",
    "\n",
    "    I, J = np.unravel_index(np.arange(n_clusters**2), (n_clusters, n_clusters))\n",
    "    # for every class\n",
    "    for i, c_i in enumerate(centroids):\n",
    "\n",
    "        # get normalized difference vectors between cluster centers\n",
    "        diff_i = get_difference_vectors(c_i)\n",
    "        diff_i_t = torch.Tensor(diff_i)\n",
    "\n",
    "\n",
    "        bestdots = np.zeros(diff_i.shape[0])\n",
    "        bestdotidx = np.zeros((diff_i.shape[0],2),dtype=int)\n",
    "\n",
    "        # for every other class\n",
    "        for j, c_j in enumerate(centroids):\n",
    "            if i==j:\n",
    "                continue\n",
    "            print(i,j)\n",
    "\n",
    "            # get normalized difference vectors\n",
    "            diff_j = get_difference_vectors(c_j)\n",
    "            diff_j = torch.Tensor(diff_j)\n",
    "\n",
    "            #compute cosine distance and take the maximum\n",
    "            dots = diff_i_t.mm(diff_j.transpose(0,1))\n",
    "            maxdots, argmaxdots = dots.max(1)\n",
    "            maxdots = maxdots.cpu().numpy().reshape(-1)\n",
    "            argmaxdots = argmaxdots.cpu().numpy().reshape(-1)\n",
    "\n",
    "            # if maximum is better than best seen so far, update\n",
    "            betteridx = maxdots>bestdots\n",
    "            bestdots[betteridx] = maxdots[betteridx]\n",
    "            bestdotidx[betteridx,0] = j*n_clusters + I[argmaxdots[betteridx]]\n",
    "            bestdotidx[betteridx,1] = j*n_clusters + J[argmaxdots[betteridx]]\n",
    "\n",
    "\n",
    "        # store discovered analogies\n",
    "        stop = start+diff_i.shape[0]\n",
    "        analogies[start : stop,0]=i*n_clusters + I\n",
    "        analogies[start : stop,1]=i*n_clusters + J\n",
    "        analogies[start : stop,2:] = bestdotidx\n",
    "        analogy_scores[start : stop] = bestdots\n",
    "        start = stop\n",
    "\n",
    "    #prune away trivial analogies\n",
    "    good_analogies = (analogy_scores>0) & (analogies[:,0]!=analogies[:,1]) & (analogies[:,2]!=analogies[:,3])\n",
    "    return analogies[good_analogies,:], analogy_scores[good_analogies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "0 5\n",
      "0 6\n",
      "0 7\n",
      "0 8\n",
      "1 0\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "1 5\n",
      "1 6\n",
      "1 7\n",
      "1 8\n",
      "2 0\n",
      "2 1\n",
      "2 3\n",
      "2 4\n",
      "2 5\n",
      "2 6\n",
      "2 7\n",
      "2 8\n",
      "3 0\n",
      "3 1\n",
      "3 2\n",
      "3 4\n",
      "3 5\n",
      "3 6\n",
      "3 7\n",
      "3 8\n",
      "4 0\n",
      "4 1\n",
      "4 2\n",
      "4 3\n",
      "4 5\n",
      "4 6\n",
      "4 7\n",
      "4 8\n",
      "5 0\n",
      "5 1\n",
      "5 2\n",
      "5 3\n",
      "5 4\n",
      "5 6\n",
      "5 7\n",
      "5 8\n",
      "6 0\n",
      "6 1\n",
      "6 2\n",
      "6 3\n",
      "6 4\n",
      "6 5\n",
      "6 7\n",
      "6 8\n",
      "7 0\n",
      "7 1\n",
      "7 2\n",
      "7 3\n",
      "7 4\n",
      "7 5\n",
      "7 6\n",
      "7 8\n",
      "8 0\n",
      "8 1\n",
      "8 2\n",
      "8 3\n",
      "8 4\n",
      "8 5\n",
      "8 6\n",
      "8 7\n"
     ]
    }
   ],
   "source": [
    "analogies, analogy_scores = mine_analogies(centroids)\n",
    "np.save('analogies.npy', analogies.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(filehandle, base_classes, cachefile, networkfile, total_num_classes=14, lr=0.1, wd=0.0001, momentum=0.9, batchsize=20, niter=500):\n",
    "    # either use pre-existing classifier or train one\n",
    "    all_labels = filehandle['all_labels'][...]\n",
    "    all_labels = all_labels.astype(int)\n",
    "    all_feats = filehandle['all_feats']\n",
    "    base_class_ids = np.where(np.in1d(all_labels, base_classes))[0]\n",
    "    #print(base_class_ids)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    model = nn.Linear(all_feats[0].size, total_num_classes)\n",
    "    if os.path.isfile(cachefile):\n",
    "        tmp = torch.load(cachefile)\n",
    "        model.load_state_dict(tmp)\n",
    "    elif os.path.isfile(networkfile):\n",
    "        tmp = torch.load(networkfile)\n",
    "        if 'module.classifier.bias' in tmp['state']:\n",
    "            state_dict = {'weight':tmp['state']['module.classifier.weight'], 'bias':tmp['state']['module.classifier.bias']}\n",
    "        else:\n",
    "            model = nn.Linear(all_feats[0].size, total_num_classes, bias=False).cuda()\n",
    "            state_dict = {'weight':tmp['state']['module.classifier.weight']}\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr, momentum=momentum, weight_decay=wd, dampening=0)\n",
    "        for i in range(niter):\n",
    "            optimizer.zero_grad()\n",
    "            idx = np.sort(np.random.choice(base_class_ids, batchsize, replace=False))\n",
    "            F = all_feats[idx,:]\n",
    "            F = Variable(torch.Tensor(F))\n",
    "            L = Variable(torch.LongTensor(all_labels[idx]))\n",
    "            S = model(F)\n",
    "            loss_val = loss(S, L)\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                print('Classifier training {:d}: {:f}'.format(i, loss_val.data[0]))\n",
    "        torch.save(model.state_dict(), cachefile)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier training 0: 2.660238\n",
      "Classifier training 100: 0.047393\n",
      "Classifier training 200: 0.039515\n",
      "Classifier training 300: 0.003187\n",
      "Classifier training 400: 0.024908\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('resnet_features.hdf5', 'r') as features_file:\n",
    "    classification_model = train_classifier(features_file, base_classes, 'classifier.pkl', 'random.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy regressor train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalogyRegressor(nn.Module):\n",
    "    def __init__(self, featdim, innerdim=512):\n",
    "        super(AnalogyRegressor,self).__init__()\n",
    "        self.featdim = featdim\n",
    "        self.innerdim = innerdim\n",
    "        self.fc1 = nn.Linear(featdim*3, innerdim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(innerdim, innerdim)\n",
    "        self.fc3 = nn.Linear(innerdim, featdim)\n",
    "\n",
    "    def forward(self, a,c,d):\n",
    "        x = torch.cat((a,c,d), dim=1)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "def train_analogy_regressor(analogies, centroids, base_classes, trained_classifier, lr=0.1, wt=10, niter=3000, step_after=5000, batchsize=40, momentum=0.9, wd=0.0001):\n",
    "    # pre-permute analogies\n",
    "    permuted_analogies = analogies[np.random.permutation(analogies.shape[0])]\n",
    "\n",
    "    # create model and init\n",
    "    featdim = centroids[0].shape[1]\n",
    "    model = AnalogyRegressor(featdim)\n",
    "    model = model\n",
    "    trained_classifier = trained_classifier\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr, momentum=momentum, weight_decay=wd, dampening=momentum)\n",
    "    loss_1 = nn.CrossEntropyLoss()\n",
    "    loss_2 = nn.MSELoss()\n",
    "\n",
    "\n",
    "    num_clusters_per_class = centroids[0].shape[0]\n",
    "    centroid_labels = (np.array(base_classes).reshape((-1,1)) * np.ones((1, num_clusters_per_class))).reshape(-1)\n",
    "    concatenated_centroids = np.concatenate(centroids, axis=0)\n",
    "\n",
    "\n",
    "    start=0\n",
    "    avg_loss_1 = avg_loss_2 = count = 0.0\n",
    "    for i in range(niter):\n",
    "        # get current batch of analogies\n",
    "        stop = min(start+batchsize, permuted_analogies.shape[0])\n",
    "        #print(start+batchsize, permuted_analogies.shape[0])\n",
    "        to_train = permuted_analogies[start:stop,:]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # analogy is A:B :: C:D, goal is to predict B from A, C, D\n",
    "        # Y is the class label of B (and A)\n",
    "        A = concatenated_centroids[to_train[:,0]]\n",
    "        B = concatenated_centroids[to_train[:,1]]\n",
    "        C = concatenated_centroids[to_train[:,2]]\n",
    "        D = concatenated_centroids[to_train[:,3]]\n",
    "        Y = centroid_labels[to_train[:,1]]\n",
    "\n",
    "        A = Variable(torch.Tensor(A))\n",
    "        B = Variable(torch.Tensor(B))\n",
    "        C = Variable(torch.Tensor(C))\n",
    "        D = Variable(torch.Tensor(D))\n",
    "        Y = Variable(torch.LongTensor(Y.astype(int)))\n",
    "\n",
    "        Bhat = model(A,C,D)\n",
    "\n",
    "        lossval_2 = loss_2(Bhat, B) # simple mean squared error loss\n",
    "\n",
    "        # classification loss\n",
    "        predicted_classprobs = trained_classifier(Bhat)\n",
    "        lossval_1 = loss_1(predicted_classprobs, Y)\n",
    "        loss = lossval_1 + wt * lossval_2\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss_1 = avg_loss_1 + lossval_1.data[0]\n",
    "        avg_loss_2 = avg_loss_2 + lossval_2.data[0]\n",
    "        count = count+1.0\n",
    "\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('{:d} : {:f}, {:f}, {:f}'.format(i, avg_loss_1/count, avg_loss_2/count, count))\n",
    "            avg_loss_1 = avg_loss_2 = count = 0.0\n",
    "\n",
    "        if (i+1) % step_after == 0:\n",
    "            lr = lr / 10.0\n",
    "            print(lr)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "        start = stop\n",
    "        if start==permuted_analogies.shape[0]:\n",
    "            start=0\n",
    "\n",
    "    return dict(model_state=model.state_dict(), concatenated_centroids=torch.Tensor(concatenated_centroids),\n",
    "            num_base_classes=len(centroids), num_clusters_per_class=num_clusters_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "print(analogies.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 3.957476, 0.454064, 1.000000\n",
      "50 : 4.274689, 0.423214, 50.000000\n",
      "100 : 0.962870, 0.244093, 50.000000\n",
      "150 : 0.237396, 0.162183, 50.000000\n",
      "200 : 0.001571, 0.111788, 50.000000\n",
      "250 : 0.000898, 0.096564, 50.000000\n",
      "300 : 0.000764, 0.089482, 50.000000\n",
      "350 : 0.000681, 0.084209, 50.000000\n",
      "400 : 0.000541, 0.081249, 50.000000\n",
      "450 : 0.000435, 0.079073, 50.000000\n",
      "500 : 0.000378, 0.077413, 50.000000\n",
      "550 : 0.000344, 0.075729, 50.000000\n",
      "600 : 0.000306, 0.073976, 50.000000\n",
      "650 : 0.000280, 0.072458, 50.000000\n",
      "700 : 0.000248, 0.071526, 50.000000\n",
      "750 : 0.000226, 0.070863, 50.000000\n",
      "800 : 0.000221, 0.070271, 50.000000\n",
      "850 : 0.000204, 0.069605, 50.000000\n",
      "900 : 0.000189, 0.068854, 50.000000\n",
      "950 : 0.000183, 0.068201, 50.000000\n",
      "1000 : 0.000175, 0.067466, 50.000000\n",
      "1050 : 0.000179, 0.066125, 50.000000\n",
      "1100 : 0.000170, 0.065601, 50.000000\n",
      "1150 : 0.000161, 0.065168, 50.000000\n",
      "1200 : 0.000154, 0.064760, 50.000000\n",
      "1250 : 0.000147, 0.064369, 50.000000\n",
      "1300 : 0.000141, 0.063993, 50.000000\n",
      "1350 : 0.000134, 0.063635, 50.000000\n",
      "1400 : 0.000128, 0.063288, 50.000000\n",
      "1450 : 0.000125, 0.062954, 50.000000\n",
      "1500 : 0.000120, 0.062614, 50.000000\n",
      "1550 : 0.000120, 0.061252, 50.000000\n",
      "1600 : 0.000116, 0.060708, 50.000000\n",
      "1650 : 0.000113, 0.060385, 50.000000\n",
      "1700 : 0.000110, 0.060099, 50.000000\n",
      "1750 : 0.000107, 0.059819, 50.000000\n",
      "1800 : 0.000102, 0.058766, 50.000000\n",
      "1850 : 0.000101, 0.058022, 50.000000\n",
      "1900 : 0.000101, 0.057756, 50.000000\n",
      "1950 : 0.000101, 0.057528, 50.000000\n",
      "2000 : 0.000102, 0.057322, 50.000000\n",
      "2050 : 0.000103, 0.057110, 50.000000\n",
      "2100 : 0.000119, 0.056480, 50.000000\n",
      "2150 : 0.000114, 0.055848, 50.000000\n",
      "2200 : 0.000113, 0.054627, 50.000000\n",
      "2250 : 0.000111, 0.054199, 50.000000\n",
      "2300 : 0.000111, 0.054008, 50.000000\n",
      "2350 : 0.000111, 0.053845, 50.000000\n",
      "2400 : 0.000111, 0.053700, 50.000000\n",
      "2450 : 0.000111, 0.053567, 50.000000\n",
      "2500 : 0.000111, 0.053444, 50.000000\n",
      "2550 : 0.000111, 0.053324, 50.000000\n",
      "2600 : 0.000102, 0.053044, 50.000000\n",
      "2650 : 0.000085, 0.052362, 50.000000\n",
      "2700 : 0.000085, 0.052237, 50.000000\n",
      "2750 : 0.000087, 0.052134, 50.000000\n",
      "2800 : 0.000086, 0.052025, 50.000000\n",
      "2850 : 0.000087, 0.051921, 50.000000\n",
      "2900 : 0.000087, 0.051824, 50.000000\n",
      "2950 : 0.000087, 0.051733, 50.000000\n"
     ]
    }
   ],
   "source": [
    "generator = train_analogy_regressor(analogies, centroids, base_classes, classification_model, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(generator['num_base_classes'])\n",
    "print(generator['concatenated_centroids'].size(1))\n",
    "#torch.save(generator, open('generator.tar','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_generator():\n",
    "#     G = torch.load(open('generator.tar'), 'rb')\n",
    "    featdim = generator['concatenated_centroids'].size(1)\n",
    "    model = AnalogyRegressor(featdim)\n",
    "    model.load_state_dict(generator['model_state'])\n",
    "#    G['model'] =model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_generate(feats, labels, generator, gen_model, max_per_label):\n",
    "    # generate till there are at least max_per_label examples for each label\n",
    "    unique_labels = np.unique(labels)\n",
    "    print(unique_labels)\n",
    "    generations_needed = []\n",
    "    generator['concatenated_centroids'] = generator['concatenated_centroids']\n",
    "    for k, lab in enumerate(unique_labels):\n",
    "        # for each label\n",
    "        idx = np.where(labels==lab)[0]\n",
    "        print(idx.size)\n",
    "        # generate this many examples:\n",
    "        num_to_gen = max(max_per_label - idx.size,0)\n",
    "        \n",
    "        if num_to_gen>0:\n",
    "            # choose a random seed\n",
    "            seed = np.random.choice(idx, num_to_gen)\n",
    "            # and a random base class\n",
    "            base_class = np.random.choice(generator['num_base_classes'], num_to_gen)\n",
    "            # and two random centroids from this base class\n",
    "            c_c = np.random.choice(generator['num_clusters_per_class'], num_to_gen)\n",
    "            c_d = np.random.choice(generator['num_clusters_per_class'], num_to_gen)\n",
    "\n",
    "            centroid_ids_c = base_class*generator['num_clusters_per_class'] + c_c\n",
    "            centroid_ids_d = base_class*generator['num_clusters_per_class'] + c_d\n",
    "            # add to list of things to generate\n",
    "            generations_needed.append( np.concatenate((seed.reshape((-1,1)), centroid_ids_c.reshape((-1,1)), centroid_ids_d.reshape((-1,1))),axis=1))\n",
    "\n",
    "    if len(generations_needed)>0:\n",
    "        generations_needed = np.concatenate(generations_needed, axis=0)\n",
    "        gen_feats = np.zeros((generations_needed.shape[0],feats.shape[1]))\n",
    "        gen_labels = np.zeros(generations_needed.shape[0])\n",
    "\n",
    "\n",
    "        # batch up the generations\n",
    "        batchsize=20\n",
    "        for start in range(0, generations_needed.shape[0], batchsize):\n",
    "            stop = min(start + batchsize, generations_needed.shape[0])\n",
    "            g_idx = generations_needed[start:stop,:]\n",
    "            A = Variable(torch.Tensor(feats[g_idx[:,0],:]))\n",
    "            C = Variable(torch.Tensor(generator['concatenated_centroids'][g_idx[:,1],:]))\n",
    "            D = Variable(torch.Tensor(generator['concatenated_centroids'][g_idx[:,2],:]))\n",
    "            F = gen_model(A,C,D).cpu().data.numpy().copy()\n",
    "            gen_feats[start:stop,:] = F\n",
    "            print(np.linalg.norm(F-feats[g_idx[:,0],:]), np.linalg.norm(F), np.linalg.norm(feats[g_idx[:,0],:]))\n",
    "            gen_labels[start:stop] = labels[g_idx[:,0]]\n",
    "\n",
    "        return np.concatenate((feats, gen_feats), axis=0), np.concatenate((labels, gen_labels), axis=0)\n",
    "    else:\n",
    "        return feats, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "37.71373 64.06834 72.00206\n",
      "39.251198 59.015026 75.34595\n",
      "32.94492 60.190857 66.40843\n",
      "31.743113 58.34321 62.085266\n",
      "33.535183 60.09323 64.57061\n",
      "32.890892 58.827156 65.2942\n",
      "36.558365 57.91354 71.832565\n",
      "36.727913 62.4228 71.87672\n",
      "38.65991 62.796574 72.86085\n",
      "33.7015 64.69294 70.049774\n",
      "35.24881 63.1426 68.47793\n",
      "17.64901 31.978945 33.336136\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('resnet_features.hdf5', 'r') as filehandle:\n",
    "    all_labels = filehandle['all_labels'][...]\n",
    "    #all_labels = all_labels.astype(int)\n",
    "    all_feats = filehandle['all_feats']\n",
    "    novel_class_ids = np.where(np.in1d(all_labels, novel_classes))[0]\n",
    "    novel_features = all_feats[novel_class_ids, :]\n",
    "    novel_labels = all_labels[novel_class_ids]\n",
    "    gen_model = init_generator()\n",
    "    gen_features, gen_labels = do_generate(novel_features, novel_labels, generator, gen_model, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/7\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.ImageFolder(root='../novel-data/test',\n",
    "                                           transform=data_transform)\n",
    "test_dataset_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                             batch_size=20, shuffle=True,\n",
    "                                             num_workers=4)\n",
    "#pics, cls = next(iter(test_dataset_loader))\n",
    "#class_names = test_dataset.classes\n",
    "#for x in cls:\n",
    "#    print(x, class_names[x])\n",
    "save_features(net, test_dataset_loader, 'resnet_features_test.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_class_accuracy(mdl, X, Y):\n",
    "    # TODO: why can't we call .data.numpy() for train_acc as a whole?\n",
    "    outputs = mdl(X)\n",
    "    max_vals, max_indices = torch.max(outputs,1)\n",
    "    acc = (max_indices == Y).sum().data.numpy()/max_indices.size()[0]\n",
    "    print(max_indices, Y)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('resnet_features.hdf5', 'r') as filehandle:\n",
    "    all_labels = filehandle['all_labels'][...]\n",
    "    all_labels = all_labels.astype(int)\n",
    "    all_feats = filehandle['all_feats']\n",
    "    base_class_ids = np.where(np.in1d(all_labels, base_classes))[0]\n",
    "    novel_class_ids = np.where(np.in1d(all_labels, novel_classes))[0]\n",
    "    print(novel_class_ids)\n",
    "    idx = np.sort(np.random.choice(novel_class_ids, 25, replace=False))\n",
    "    F = all_feats[idx,:]\n",
    "    F = Variable(torch.Tensor(F))\n",
    "    L = Variable(torch.LongTensor(all_labels[idx]))\n",
    "    print(calc_class_accuracy(classification_model, F, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base_novel_classifier(features, labels, lr=0.1, wd=0.0001, momentum=0.9):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    model = nn.Linear(features[0].size, 6)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr, momentum=momentum, weight_decay=wd, dampening=0)\n",
    "    for i in range(500):\n",
    "        optimizer.zero_grad()\n",
    "        idx = np.sort(np.random.choice(np.arange(250), 20, replace=False))\n",
    "        #print(idx)\n",
    "        F = features[idx,:]\n",
    "        F = Variable(torch.Tensor(features))\n",
    "        L = Variable(torch.LongTensor(labels))\n",
    "        S = model(F)\n",
    "        #print(calc_class_accuracy(model, F, L))\n",
    "        loss_val = loss(S, L)\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_classifier = train_base_novel_classifier(gen_features, gen_labels)\n",
    "#print(gen_labels.shape)\n",
    "#print(calc_class_accuracy(classification_model, F, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   7   8  12  16  20  21  22  25  29  32  37  38  39  42  47  54\n",
      "  55  57  58  62  63  64  67  72  75  78  89  90  93  96  98 100 103 108\n",
      " 109 110 111 112 117 120 122 124 125 128 129 136 138]\n",
      "Variable containing:\n",
      " 2\n",
      " 1\n",
      " 2\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 1\n",
      " 4\n",
      " 2\n",
      " 1\n",
      " 3\n",
      " 4\n",
      " 3\n",
      " 4\n",
      " 4\n",
      " 4\n",
      " 5\n",
      " 1\n",
      " 2\n",
      " 4\n",
      " 4\n",
      " 4\n",
      " 2\n",
      " 4\n",
      " 2\n",
      "[torch.LongTensor of size 25]\n",
      " Variable containing:\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 4\n",
      " 1\n",
      " 4\n",
      " 3\n",
      " 4\n",
      " 1\n",
      " 3\n",
      " 5\n",
      " 3\n",
      " 5\n",
      " 2\n",
      " 3\n",
      " 1\n",
      " 5\n",
      " 3\n",
      " 3\n",
      " 1\n",
      " 2\n",
      " 4\n",
      " 3\n",
      " 2\n",
      " 1\n",
      "[torch.LongTensor of size 25]\n",
      "\n",
      "[0.16]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('resnet_features_test.hdf5', 'r') as filehandle:\n",
    "    all_labels = filehandle['all_labels'][...]\n",
    "    all_labels = all_labels.astype(int)\n",
    "    all_feats = filehandle['all_feats']\n",
    "    novel_class_ids = np.where(np.in1d(all_labels, novel_classes))[0]\n",
    "    print(novel_class_ids)\n",
    "    idx = np.sort(np.random.choice(novel_class_ids, 25, replace=False))\n",
    "    F = all_feats[idx,:]\n",
    "    F = Variable(torch.Tensor(F))\n",
    "    L = Variable(torch.LongTensor(all_labels[idx]))\n",
    "    print(calc_class_accuracy(novel_classifier, F, L))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
