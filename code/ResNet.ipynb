{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic ResNet model\n",
    "\n",
    "def init_layer(L):\n",
    "    # Initialization using fan-in\n",
    "    if isinstance(L, nn.Conv2d):\n",
    "        n = L.kernel_size[0]*L.kernel_size[1]*L.out_channels\n",
    "        L.weight.data.normal_(0,math.sqrt(2.0/float(n)))\n",
    "    elif isinstance(L, nn.BatchNorm2d):\n",
    "        L.weight.data.fill_(1)\n",
    "        L.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Simple ResNet Block\n",
    "class SimpleBlock(nn.Module):\n",
    "    def __init__(self, indim, outdim, half_res):\n",
    "        super(SimpleBlock, self).__init__()\n",
    "        self.indim = indim\n",
    "        self.outdim = outdim\n",
    "        self.C1 = nn.Conv2d(indim, outdim, kernel_size=3, stride=2 if half_res else 1, padding=1, bias=False)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.BN1 = nn.BatchNorm2d(outdim)\n",
    "        self.C2 = nn.Conv2d(outdim, outdim,kernel_size=3, padding=1,bias=False)\n",
    "        self.BN2 = nn.BatchNorm2d(outdim)\n",
    "\n",
    "        self.parametrized_layers = [self.C1, self.C2, self.BN1, self.BN2]\n",
    "\n",
    "        self.half_res = half_res\n",
    "\n",
    "        # if the input number of channels is not equal to the output, then need a 1x1 convolution\n",
    "        if indim!=outdim:\n",
    "            self.shortcut = nn.Conv2d(indim, outdim, 1, 2 if half_res else 1, bias=False)\n",
    "            self.parametrized_layers.append(self.shortcut)\n",
    "            self.BNshortcut = nn.BatchNorm2d(outdim)\n",
    "            self.parametrized_layers.append(self.BNshortcut)\n",
    "            self.shortcut_type = '1x1'\n",
    "        else:\n",
    "            self.shortcut_type = 'identity'\n",
    "\n",
    "        for layer in self.parametrized_layers:\n",
    "            init_layer(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.C1(x)\n",
    "        out = self.BN1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.C2(out)\n",
    "        out = self.BN2(out)\n",
    "        short_out = x if self.shortcut_type == 'identity' else self.BNshortcut(self.shortcut(x))\n",
    "        out = out + short_out\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Bottleneck block\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, indim, outdim, half_res):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        bottleneckdim = int(outdim/4)\n",
    "        self.indim = indim\n",
    "        self.outdim = outdim\n",
    "        self.C1 = nn.Conv2d(indim, bottleneckdim, kernel_size=1,  bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.BN1 = nn.BatchNorm2d(bottleneckdim)\n",
    "        self.C2 = nn.Conv2d(bottleneckdim, bottleneckdim, kernel_size=3, stride=2 if half_res else 1,padding=1)\n",
    "        self.BN2 = nn.BatchNorm2d(bottleneckdim)\n",
    "        self.C3 = nn.Conv2d(bottleneckdim, outdim, kernel_size=1, bias=False)\n",
    "        self.BN3 = nn.BatchNorm2d(outdim)\n",
    "\n",
    "        self.parametrized_layers = [self.C1, self.BN1, self.C2, self.BN2, self.C3, self.BN3]\n",
    "        self.half_res = half_res\n",
    "\n",
    "\n",
    "        # if the input number of channels is not equal to the output, then need a 1x1 convolution\n",
    "        if indim!=outdim:\n",
    "            self.shortcut = nn.Conv2d(indim, outdim, 1, stride=2 if half_res else 1, bias=False)\n",
    "            self.parametrized_layers.append(self.shortcut)\n",
    "            self.shortcut_type = '1x1'\n",
    "        else:\n",
    "            self.shortcut_type = 'identity'\n",
    "\n",
    "        for layer in self.parametrized_layers:\n",
    "            init_layer(layer)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        short_out = x if self.shortcut_type == 'identity' else self.shortcut(x)\n",
    "        out = self.C1(x)\n",
    "        out = self.BN1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.C2(out)\n",
    "        out = self.BN2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.C3(out)\n",
    "        out = self.BN3(out)\n",
    "        out = out + short_out\n",
    "\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,block,list_of_num_layers, list_of_out_dims, num_classes=1000, only_trunk=False ):\n",
    "        # list_of_num_layers specifies number of layers in each stage\n",
    "        # list_of_out_dims specifies number of output channel for each stage\n",
    "        super(ResNet,self).__init__()\n",
    "        self.grads = []\n",
    "        self.fmaps = []\n",
    "        assert len(list_of_num_layers)==4, 'Can have only four stages'\n",
    "        conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                               bias=False)\n",
    "        bn1 = nn.BatchNorm2d(64)\n",
    "        relu = nn.ReLU()\n",
    "        pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        init_layer(conv1)\n",
    "        init_layer(bn1)\n",
    "\n",
    "\n",
    "        trunk = [conv1, bn1, relu, pool1]\n",
    "        indim = 64\n",
    "        for i in range(4):\n",
    "\n",
    "            for j in range(list_of_num_layers[i]):\n",
    "                half_res = (i>=1) and (j==0)\n",
    "                B = block(indim, list_of_out_dims[i], half_res)\n",
    "                trunk.append(B)\n",
    "                indim = list_of_out_dims[i]\n",
    "\n",
    "\n",
    "\n",
    "        self.only_trunk=only_trunk\n",
    "        if not only_trunk:\n",
    "            avgpool = nn.AvgPool2d(7)\n",
    "            trunk.append(avgpool)\n",
    "\n",
    "        self.trunk = nn.Sequential(*trunk)\n",
    "        self.final_feat_dim = indim\n",
    "        if not only_trunk:\n",
    "            self.classifier = nn.Linear(indim, num_classes)\n",
    "            self.classifier.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.trunk(x)\n",
    "        if self.only_trunk:\n",
    "            return out\n",
    "        out = out.view(out.size(0),-1)\n",
    "        scores = self.classifier(out)\n",
    "        return scores\n",
    "\n",
    "\n",
    "def ResNet10(num_classes=1000, only_trunk=False):\n",
    "    return ResNet(SimpleBlock, [1,1,1,1],[64,128,256,512], num_classes, only_trunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((300,300)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "train_dataset = datasets.ImageFolder(root='../aligned-data/train',\n",
    "                                           transform=data_transform)\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                             batch_size=8, shuffle=True, \n",
    "                                            num_workers=4)\n",
    "test_dataset = datasets.ImageFolder(root='../aligned-data/test',\n",
    "                                           transform=data_transform)\n",
    "test_dataset_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                             batch_size=90, shuffle=True,\n",
    "                                             num_workers=4)\n",
    "test_x, test_y = next(iter(test_dataset_loader))\n",
    "test_x, test_y = Variable(test_x), Variable(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e2, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(mdl, X, Y):\n",
    "    # TODO: why can't we call .data.numpy() for train_acc as a whole?\n",
    "    outputs = mdl(X)\n",
    "    max_vals, max_indices = torch.max(outputs,1)\n",
    "    train_acc = (max_indices == Y).sum().data.numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet10(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  51.688549280166626\n",
      "Training accuracy:  [0.12173913]\n",
      "Test accuracy:  [0.15555556]\n",
      "Loss:  51.80729579925537\n",
      "Training accuracy:  [0.09130435]\n",
      "Test accuracy:  [0.15555556]\n",
      "Loss:  51.83555364608765\n",
      "Training accuracy:  [0.1]\n",
      "Test accuracy:  [0.15555556]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-1c2ed0895b4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 99\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    acc = 0\n",
    "    running_loss = 0.0\n",
    "    test_acc = 0\n",
    "    train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                             batch_size=20, shuffle=True,\n",
    "                                             num_workers=4)\n",
    "    \n",
    "    for i, data in enumerate(train_dataset_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc += calc_accuracy(net, inputs, labels)\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        #if i % 5 == 4:    # print every 2000 mini-batches\n",
    "            #print('[%d, %5d] loss: %.3f' %\n",
    "            #      (epoch + 1, i + 1, running_loss / 5))\n",
    "        #    running_loss = 0.0\n",
    "        \n",
    "    # Compute test accuracy\n",
    "    test_acc += calc_accuracy(net, test_x, test_y)\n",
    "\n",
    "    print('Loss: ', running_loss)\n",
    "    print('Training accuracy: ', acc/(len(train_dataset_loader)))\n",
    "    print('Test accuracy: ', test_acc)\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
